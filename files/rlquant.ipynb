{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e7d93-3b1b-43bf-9602-54711f568cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3d19a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+rocm5.4.2\n"
     ]
    }
   ],
   "source": [
    "# PyTorch basic Libraries \n",
    "import torch                                        # root package\n",
    "# check version \n",
    "print(torch.__version__)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "# from torch.jit import script, trace       # hybrid frontend decorator and tracing jit\n",
    "\n",
    "#  Classic tools \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Setup device dignostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set the device globally\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# For classification problems\n",
    "# from  torchmetrics import Accuracy \n",
    "# torchmetric_accuracy = Accuracy().to(device)  \n",
    "\n",
    "# Computer Vision \n",
    "# import torchvision \n",
    "# from torchvision import datasets \n",
    "# from torchvision import transforms \n",
    "# from torchvision.transforms import ToTensor \n",
    "\n",
    "# print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e3a9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackProcess:\n",
    "    def __init__(self, S0, r, sigma, n):\n",
    "        self.S0 = S0\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "        self.n = n\n",
    "\n",
    "    def generate(self):\n",
    "        S0, r, sigma, n = self.S0, self.r, self.sigma, self.n\n",
    "        dt = 1 / 365\n",
    "        dW = np.random.normal(0, dt ** 0.5, n)\n",
    "        chg = np.ones(n + 1)\n",
    "        chg[1:] += r * dt + sigma * dW\n",
    "        accum_chg = chg.cumprod()\n",
    "        return S0 * accum_chg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acdcb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaEnv():\n",
    "    n_observation = 5\n",
    "\n",
    "    def __init__(self, process: BlackProcess, tenor, strike):\n",
    "        self.process = process\n",
    "        self.tenor = tenor\n",
    "        self.strike = strike\n",
    "        self.t = 0\n",
    "        self.path = None\n",
    "        self.observations = None\n",
    "        self.reset()\n",
    "\n",
    "    def df(self):\n",
    "        return exp(-self.process.r / 365)\n",
    "\n",
    "    def mu(self):\n",
    "        return exp(self.process.r / 365) - 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.path = self.process.generate()\n",
    "        self.t = 0\n",
    "        self.observations = np.stack([self.observation(t) for t in range(self.tenor + 1)], 0)\n",
    "        return self.observations[0]\n",
    "\n",
    "    def St(self, t=None) -> np.float32:\n",
    "        t = self.t if t is None else t\n",
    "        return self.path[t]\n",
    "\n",
    "    def observation(self, t=None):\n",
    "        S_K = self.St(t) / self.strike\n",
    "        moneyness = max(0, S_K)\n",
    "\n",
    "        t = self.t if t is None else t\n",
    "        tenor = (self.tenor - t) / 365\n",
    "\n",
    "        obs = np.array([moneyness, moneyness ** 2, tenor, tenor ** 2, moneyness * tenor])\n",
    "        assert len(obs) == self.n_observation\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: hedge ratio, i.e. delta\n",
    "        :return: S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "        \"\"\"\n",
    "        S_t0 = self.observations[self.t]\n",
    "        self.t = self.t + 1\n",
    "        dS = self.St() - self.St(self.t - 1)\n",
    "        reward = dS * action\n",
    "        S_t1 = self.observations[self.t]\n",
    "        terminated = True if self.t >= self.tenor else False\n",
    "        can_early_exercise = False\n",
    "        payoff = self.payoff()\n",
    "        return S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "\n",
    "    def payoff(self, t=None) -> np.float32:\n",
    "        \"\"\"\n",
    "        :return: option payoff if exercise now, regardless it can be exercised, equivalent to moneyless\n",
    "        \"\"\"\n",
    "        return max(0, self.St(t) - self.strike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd9348bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0, r, vol, days, strike = 1, 0.01, 0.3, 30, 1.1\n",
    "n_samples = 2 ** 12\n",
    "#n_hidden = [64, 64]\n",
    "n_hidden = [8, 8]\n",
    "process = BlackProcess(S0, r, vol, days)\n",
    "N_OBSERVATION = VanillaEnv.n_observation\n",
    "env = VanillaEnv(process, days, strike)\n",
    "#print(\"pretrain actor\")\n",
    "#actor = get_pretrain_actor(env, n_hidden, n_samples)\n",
    "#env = VanillaEnv(process, days, S0)\n",
    "#print(\"pretrain critic\")\n",
    "#critic, _ = get_pretrain_critic(env, actor, n_hidden, N_OBSERVATION, n_samples, epoc=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2ee40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(n_hidden, N_OBS): \n",
    "    layers = nn.ModuleList() \n",
    "    param = [N_OBS] + n_hidden \n",
    "    for i in range(len(param)): \n",
    "            if i != (len(param)-1):\n",
    "                layers.append(nn.Linear(in_features=param[i], out_features=param[(i+1)]))\n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                layers.append(nn.Linear(in_features=param[i], out_features=1))\n",
    "                layers.append(nn.Tanh())\n",
    "    return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "def get_critic(n_hidden, N_OBS): \n",
    "    layers = nn.ModuleList() \n",
    "    param = [N_OBS] + n_hidden \n",
    "    for i in range(len(param)): \n",
    "            if i != (len(param)-1):\n",
    "                layers.append(nn.Linear(in_features=param[i], out_features=param[(i+1)]))\n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                layers.append(nn.Linear(in_features=param[i], out_features=1))\n",
    "                layers.append(nn.Sigmoid())\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f193785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preTrainer(nn.Module): \n",
    "    def __init__(self, N_OBS): \n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1_1 = nn.Linear(N_OBS, 8)\n",
    "        self.layer1_2 = nn.Linear(8, 8)\n",
    "        self.layer1_3 = nn.Linear(8,1)\n",
    "\n",
    "\n",
    "    def forward(self, x, y): \n",
    "        \"\"\" \n",
    "        x, y are tensors of type (None, days, N_OBS)\n",
    "        \"\"\"\n",
    "        #print(x.shape, y.shape)\n",
    "        x = self.layer1_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.layer1_2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.layer1_3(x)\n",
    "        #print(x.shape)\n",
    "        return (x*y).sum(dim=(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59906031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_123 = preTrainer(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d57986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3800, -2.1495, -2.4478, -2.2833, -2.2343], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = torch.rand(5, 30, 18)\n",
    "y_tensor = torch.rand(5, 30, 1)\n",
    "model_123(x_tensor, y_tensor)\n",
    "# preTrainer_sum(model_123, x_tensor.to(device), y_tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17e462b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a13cb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1_1.weight',\n",
       "              tensor([[ 0.1826, -0.0834, -0.1204, -0.0122,  0.0703,  0.0254, -0.0729,  0.1486,\n",
       "                       -0.1828,  0.1722, -0.2175,  0.2306,  0.2127,  0.0968, -0.2138,  0.1046,\n",
       "                        0.0435, -0.2129],\n",
       "                      [ 0.0236,  0.2288,  0.1919,  0.1323,  0.1585,  0.0833, -0.1170,  0.1745,\n",
       "                        0.0831,  0.1277, -0.1120,  0.1026, -0.0782, -0.1238,  0.0741,  0.0292,\n",
       "                       -0.0679, -0.0013],\n",
       "                      [ 0.0863, -0.0057,  0.0399,  0.0308, -0.0328,  0.0494, -0.0746, -0.1883,\n",
       "                        0.1139, -0.1220,  0.1959, -0.0488, -0.2176,  0.2320,  0.1880,  0.0361,\n",
       "                        0.0901,  0.2088],\n",
       "                      [-0.0616, -0.2311, -0.0436,  0.0084, -0.0441,  0.2217, -0.1051,  0.0415,\n",
       "                        0.1876, -0.0383,  0.2030,  0.0388, -0.1228, -0.2335,  0.0141,  0.0139,\n",
       "                        0.0938, -0.2250],\n",
       "                      [ 0.0193, -0.2352, -0.0704, -0.1460,  0.0252,  0.0840,  0.1718,  0.1104,\n",
       "                        0.0299,  0.0894,  0.0260,  0.0979, -0.0719,  0.0816,  0.2008,  0.2087,\n",
       "                       -0.0677, -0.1106],\n",
       "                      [-0.1193, -0.0362,  0.1912,  0.0500, -0.0727,  0.1504, -0.0575,  0.2255,\n",
       "                        0.1677,  0.0232, -0.1404,  0.1116,  0.0589, -0.2250, -0.0728,  0.0777,\n",
       "                       -0.1612, -0.1531],\n",
       "                      [ 0.1133,  0.1673, -0.1485,  0.1300,  0.0524,  0.0362,  0.1992, -0.1636,\n",
       "                        0.0126,  0.1103, -0.0101, -0.1033,  0.0923, -0.1412,  0.2310, -0.2058,\n",
       "                       -0.0438,  0.2335],\n",
       "                      [ 0.0470,  0.1847,  0.1280,  0.0789, -0.0229,  0.0598,  0.2211,  0.0982,\n",
       "                        0.1695, -0.1728,  0.1949,  0.1842, -0.1644,  0.0619,  0.1892,  0.1139,\n",
       "                       -0.2347,  0.1630]])),\n",
       "             ('layer1_1.bias',\n",
       "              tensor([-0.1289,  0.0794,  0.1460, -0.1329, -0.2018, -0.1321,  0.2145, -0.0500])),\n",
       "             ('layer1_2.weight',\n",
       "              tensor([[ 0.0347, -0.0768, -0.1756,  0.3035, -0.2472, -0.2157, -0.0337, -0.2481],\n",
       "                      [ 0.0908,  0.2472,  0.3097, -0.0345, -0.0215,  0.3046,  0.0284, -0.2374],\n",
       "                      [-0.0232, -0.2954, -0.0494,  0.0879, -0.0803, -0.1414, -0.2471, -0.0438],\n",
       "                      [ 0.3470,  0.0560,  0.1385, -0.1881,  0.0168, -0.2179, -0.0036, -0.1716],\n",
       "                      [-0.0493,  0.0650,  0.0155, -0.0986, -0.3238, -0.1647,  0.1270,  0.2175],\n",
       "                      [ 0.2915,  0.2603,  0.1813,  0.1078, -0.0742, -0.2418,  0.0157,  0.1648],\n",
       "                      [-0.1607,  0.0533, -0.3398, -0.1981, -0.2094,  0.1608,  0.3022,  0.3115],\n",
       "                      [-0.0397,  0.0569,  0.2270,  0.0334, -0.1795,  0.3388,  0.1704, -0.2547]])),\n",
       "             ('layer1_2.bias',\n",
       "              tensor([ 0.1353,  0.1049, -0.2530,  0.3421, -0.2908, -0.1714, -0.0052, -0.0336])),\n",
       "             ('layer1_3.weight',\n",
       "              tensor([[ 0.3171, -0.3357,  0.0922,  0.2007, -0.3346,  0.1614, -0.0263, -0.3124]])),\n",
       "             ('layer1_3.bias', tensor([-0.3303]))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = preTrainer(18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c110e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1_1.weight',\n",
       "              tensor([[ 0.1179,  0.1065,  0.0575, -0.0488, -0.2139,  0.0306, -0.1286,  0.1476,\n",
       "                       -0.0980,  0.2211, -0.0639,  0.1237, -0.1241, -0.1684,  0.2124,  0.1384,\n",
       "                       -0.0978, -0.0672],\n",
       "                      [-0.0476, -0.1924,  0.1895, -0.1690, -0.1256,  0.1083,  0.0916, -0.0957,\n",
       "                       -0.0739, -0.1065, -0.0224, -0.0470, -0.1145,  0.1737,  0.0539,  0.2310,\n",
       "                       -0.0784, -0.1509],\n",
       "                      [ 0.0867,  0.1981,  0.0640, -0.2303, -0.1681, -0.2093, -0.1398,  0.1163,\n",
       "                        0.1854, -0.2329,  0.2064,  0.2177, -0.1836,  0.1728, -0.0234,  0.0047,\n",
       "                        0.2090,  0.0661],\n",
       "                      [ 0.1968,  0.1889,  0.1439,  0.1949,  0.0516,  0.0695,  0.0140,  0.0842,\n",
       "                       -0.1309,  0.1380, -0.0463,  0.1326,  0.0887,  0.0713, -0.1435,  0.0624,\n",
       "                       -0.0010,  0.0572],\n",
       "                      [-0.0828,  0.0914, -0.2177,  0.1977,  0.1956,  0.0023, -0.2183,  0.1458,\n",
       "                        0.0727,  0.2128, -0.2346,  0.0922,  0.1520, -0.1096, -0.1018,  0.1980,\n",
       "                        0.0336,  0.0677],\n",
       "                      [-0.0272,  0.1060, -0.0786, -0.2112,  0.2355, -0.0278,  0.2086,  0.0402,\n",
       "                        0.1032, -0.1822, -0.0871,  0.0787,  0.1382, -0.2008, -0.1186,  0.1468,\n",
       "                        0.0610,  0.0842],\n",
       "                      [ 0.0253, -0.1748,  0.0229, -0.1507, -0.0149,  0.0330, -0.1560,  0.0555,\n",
       "                        0.1908, -0.0807, -0.1198, -0.0235, -0.2336,  0.0238, -0.1693,  0.0103,\n",
       "                        0.1843, -0.1231],\n",
       "                      [-0.1906,  0.1354,  0.1991, -0.1307, -0.1255, -0.2189, -0.0891,  0.2009,\n",
       "                        0.1750, -0.0021,  0.0777, -0.0595, -0.2303,  0.0084,  0.1762,  0.1510,\n",
       "                        0.0402,  0.0991]])),\n",
       "             ('layer1_1.bias',\n",
       "              tensor([ 0.0290,  0.2285, -0.2252, -0.1297, -0.2142,  0.1552, -0.1562,  0.1250])),\n",
       "             ('layer1_2.weight',\n",
       "              tensor([[ 0.0143, -0.3318, -0.0310, -0.3173, -0.3105, -0.2563,  0.2418, -0.2753],\n",
       "                      [-0.0166,  0.0948, -0.0956,  0.3081, -0.2195, -0.3437,  0.0494,  0.3355],\n",
       "                      [ 0.2691, -0.0942,  0.1688, -0.3423,  0.2526,  0.2216,  0.1045,  0.0025],\n",
       "                      [-0.0927,  0.0429,  0.2460, -0.0116,  0.2081,  0.1821,  0.0613, -0.2176],\n",
       "                      [-0.2860, -0.1986,  0.2804,  0.1284, -0.1349,  0.1544,  0.0024, -0.0205],\n",
       "                      [-0.2439,  0.0253,  0.2572,  0.0036,  0.1361, -0.1343, -0.0550,  0.0546],\n",
       "                      [-0.2533, -0.1157,  0.2916, -0.1260,  0.0229, -0.2262,  0.3047,  0.3087],\n",
       "                      [-0.1430,  0.2659, -0.0047,  0.1117, -0.1588, -0.0287, -0.1763, -0.0819]])),\n",
       "             ('layer1_2.bias',\n",
       "              tensor([ 0.0656,  0.3397, -0.2610, -0.2378, -0.1970, -0.2178,  0.0111, -0.0714])),\n",
       "             ('layer1_3.weight',\n",
       "              tensor([[ 0.0309, -0.2992, -0.0979, -0.1574,  0.2715, -0.1318,  0.2603, -0.2531]])),\n",
       "             ('layer1_3.bias', tensor([-0.0158]))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_test.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8547e972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1_1.weight',\n",
       "              tensor([[ 0.1179,  0.1065,  0.0575, -0.0488, -0.2139,  0.0306, -0.1286,  0.1476,\n",
       "                       -0.0980,  0.2211, -0.0639,  0.1237, -0.1241, -0.1684,  0.2124,  0.1384,\n",
       "                       -0.0978, -0.0672],\n",
       "                      [-0.0476, -0.1924,  0.1895, -0.1690, -0.1256,  0.1083,  0.0916, -0.0957,\n",
       "                       -0.0739, -0.1065, -0.0224, -0.0470, -0.1145,  0.1737,  0.0539,  0.2310,\n",
       "                       -0.0784, -0.1509],\n",
       "                      [ 0.0867,  0.1981,  0.0640, -0.2303, -0.1681, -0.2093, -0.1398,  0.1163,\n",
       "                        0.1854, -0.2329,  0.2064,  0.2177, -0.1836,  0.1728, -0.0234,  0.0047,\n",
       "                        0.2090,  0.0661],\n",
       "                      [ 0.1968,  0.1889,  0.1439,  0.1949,  0.0516,  0.0695,  0.0140,  0.0842,\n",
       "                       -0.1309,  0.1380, -0.0463,  0.1326,  0.0887,  0.0713, -0.1435,  0.0624,\n",
       "                       -0.0010,  0.0572],\n",
       "                      [-0.0828,  0.0914, -0.2177,  0.1977,  0.1956,  0.0023, -0.2183,  0.1458,\n",
       "                        0.0727,  0.2128, -0.2346,  0.0922,  0.1520, -0.1096, -0.1018,  0.1980,\n",
       "                        0.0336,  0.0677],\n",
       "                      [-0.0272,  0.1060, -0.0786, -0.2112,  0.2355, -0.0278,  0.2086,  0.0402,\n",
       "                        0.1032, -0.1822, -0.0871,  0.0787,  0.1382, -0.2008, -0.1186,  0.1468,\n",
       "                        0.0610,  0.0842],\n",
       "                      [ 0.0253, -0.1748,  0.0229, -0.1507, -0.0149,  0.0330, -0.1560,  0.0555,\n",
       "                        0.1908, -0.0807, -0.1198, -0.0235, -0.2336,  0.0238, -0.1693,  0.0103,\n",
       "                        0.1843, -0.1231],\n",
       "                      [-0.1906,  0.1354,  0.1991, -0.1307, -0.1255, -0.2189, -0.0891,  0.2009,\n",
       "                        0.1750, -0.0021,  0.0777, -0.0595, -0.2303,  0.0084,  0.1762,  0.1510,\n",
       "                        0.0402,  0.0991]], device='cuda:0')),\n",
       "             ('layer1_1.bias',\n",
       "              tensor([ 0.0290,  0.2285, -0.2252, -0.1297, -0.2142,  0.1552, -0.1562,  0.1250],\n",
       "                     device='cuda:0')),\n",
       "             ('layer1_2.weight',\n",
       "              tensor([[ 0.0143, -0.3318, -0.0310, -0.3173, -0.3105, -0.2563,  0.2418, -0.2753],\n",
       "                      [-0.0166,  0.0948, -0.0956,  0.3081, -0.2195, -0.3437,  0.0494,  0.3355],\n",
       "                      [ 0.2691, -0.0942,  0.1688, -0.3423,  0.2526,  0.2216,  0.1045,  0.0025],\n",
       "                      [-0.0927,  0.0429,  0.2460, -0.0116,  0.2081,  0.1821,  0.0613, -0.2176],\n",
       "                      [-0.2860, -0.1986,  0.2804,  0.1284, -0.1349,  0.1544,  0.0024, -0.0205],\n",
       "                      [-0.2439,  0.0253,  0.2572,  0.0036,  0.1361, -0.1343, -0.0550,  0.0546],\n",
       "                      [-0.2533, -0.1157,  0.2916, -0.1260,  0.0229, -0.2262,  0.3047,  0.3087],\n",
       "                      [-0.1430,  0.2659, -0.0047,  0.1117, -0.1588, -0.0287, -0.1763, -0.0819]],\n",
       "                     device='cuda:0')),\n",
       "             ('layer1_2.bias',\n",
       "              tensor([ 0.0656,  0.3397, -0.2610, -0.2378, -0.1970, -0.2178,  0.0111, -0.0714],\n",
       "                     device='cuda:0')),\n",
       "             ('layer1_3.weight',\n",
       "              tensor([[ 0.0309, -0.2992, -0.0979, -0.1574,  0.2715, -0.1318,  0.2603, -0.2531]],\n",
       "                     device='cuda:0')),\n",
       "             ('layer1_3.bias', tensor([-0.0158], device='cuda:0'))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_123.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9332bf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1_1.weight',\n",
       "              tensor([[ 0.1179,  0.1065,  0.0575, -0.0488, -0.2139,  0.0306, -0.1286,  0.1476,\n",
       "                       -0.0980,  0.2211, -0.0639,  0.1237, -0.1241, -0.1684,  0.2124,  0.1384,\n",
       "                       -0.0978, -0.0672],\n",
       "                      [-0.0476, -0.1924,  0.1895, -0.1690, -0.1256,  0.1083,  0.0916, -0.0957,\n",
       "                       -0.0739, -0.1065, -0.0224, -0.0470, -0.1145,  0.1737,  0.0539,  0.2310,\n",
       "                       -0.0784, -0.1509],\n",
       "                      [ 0.0867,  0.1981,  0.0640, -0.2303, -0.1681, -0.2093, -0.1398,  0.1163,\n",
       "                        0.1854, -0.2329,  0.2064,  0.2177, -0.1836,  0.1728, -0.0234,  0.0047,\n",
       "                        0.2090,  0.0661],\n",
       "                      [ 0.1968,  0.1889,  0.1439,  0.1949,  0.0516,  0.0695,  0.0140,  0.0842,\n",
       "                       -0.1309,  0.1380, -0.0463,  0.1326,  0.0887,  0.0713, -0.1435,  0.0624,\n",
       "                       -0.0010,  0.0572],\n",
       "                      [-0.0828,  0.0914, -0.2177,  0.1977,  0.1956,  0.0023, -0.2183,  0.1458,\n",
       "                        0.0727,  0.2128, -0.2346,  0.0922,  0.1520, -0.1096, -0.1018,  0.1980,\n",
       "                        0.0336,  0.0677],\n",
       "                      [-0.0272,  0.1060, -0.0786, -0.2112,  0.2355, -0.0278,  0.2086,  0.0402,\n",
       "                        0.1032, -0.1822, -0.0871,  0.0787,  0.1382, -0.2008, -0.1186,  0.1468,\n",
       "                        0.0610,  0.0842],\n",
       "                      [ 0.0253, -0.1748,  0.0229, -0.1507, -0.0149,  0.0330, -0.1560,  0.0555,\n",
       "                        0.1908, -0.0807, -0.1198, -0.0235, -0.2336,  0.0238, -0.1693,  0.0103,\n",
       "                        0.1843, -0.1231],\n",
       "                      [-0.1906,  0.1354,  0.1991, -0.1307, -0.1255, -0.2189, -0.0891,  0.2009,\n",
       "                        0.1750, -0.0021,  0.0777, -0.0595, -0.2303,  0.0084,  0.1762,  0.1510,\n",
       "                        0.0402,  0.0991]])),\n",
       "             ('layer1_1.bias',\n",
       "              tensor([ 0.0290,  0.2285, -0.2252, -0.1297, -0.2142,  0.1552, -0.1562,  0.1250])),\n",
       "             ('layer1_2.weight',\n",
       "              tensor([[ 0.0143, -0.3318, -0.0310, -0.3173, -0.3105, -0.2563,  0.2418, -0.2753],\n",
       "                      [-0.0166,  0.0948, -0.0956,  0.3081, -0.2195, -0.3437,  0.0494,  0.3355],\n",
       "                      [ 0.2691, -0.0942,  0.1688, -0.3423,  0.2526,  0.2216,  0.1045,  0.0025],\n",
       "                      [-0.0927,  0.0429,  0.2460, -0.0116,  0.2081,  0.1821,  0.0613, -0.2176],\n",
       "                      [-0.2860, -0.1986,  0.2804,  0.1284, -0.1349,  0.1544,  0.0024, -0.0205],\n",
       "                      [-0.2439,  0.0253,  0.2572,  0.0036,  0.1361, -0.1343, -0.0550,  0.0546],\n",
       "                      [-0.2533, -0.1157,  0.2916, -0.1260,  0.0229, -0.2262,  0.3047,  0.3087],\n",
       "                      [-0.1430,  0.2659, -0.0047,  0.1117, -0.1588, -0.0287, -0.1763, -0.0819]])),\n",
       "             ('layer1_2.bias',\n",
       "              tensor([ 0.0656,  0.3397, -0.2610, -0.2378, -0.1970, -0.2178,  0.0111, -0.0714])),\n",
       "             ('layer1_3.weight',\n",
       "              tensor([[ 0.0309, -0.2992, -0.0979, -0.1574,  0.2715, -0.1318,  0.2603, -0.2531]])),\n",
       "             ('layer1_3.bias', tensor([-0.0158]))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.load_state_dict(model_123.state_dict())\n",
    "model_test.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0970d7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "preTrainer                               [5]                       --\n",
       "├─Linear: 1-1                            [5, 30, 8]                152\n",
       "├─Linear: 1-2                            [5, 30, 8]                72\n",
       "├─Linear: 1-3                            [5, 30, 1]                9\n",
       "==========================================================================================\n",
       "Total params: 233\n",
       "Trainable params: 233\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.03\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_123, ( (5, 30, 18), (5, 30, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75d548c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Linear: 1-1                            88\n",
       "├─ReLU: 1-2                              --\n",
       "├─Linear: 1-3                            72\n",
       "├─ReLU: 1-4                              --\n",
       "├─Linear: 1-5                            9\n",
       "├─Sigmoid: 1-6                           --\n",
       "=================================================================\n",
       "Total params: 169\n",
       "Trainable params: 169\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(get_critic([8, 8], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "412fb72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30, 18), (30, 8), (30, 8)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_list = [18] + [8,8]\n",
    "days_list = [30] *3 \n",
    "param = list(zip(days_list, N_list))\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0295a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_model(n_hidden, days, N_OBS):\n",
    "    return get_actor(n_hidden, N_OBS), preTrainer(N_OBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1038c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Linear(in_features=18, out_features=8, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "   (3): ReLU()\n",
       "   (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "   (5): Tanh()\n",
       " ),\n",
       " preTrainer(\n",
       "   (layer1_1): Linear(in_features=18, out_features=8, bias=True)\n",
       "   (layer1_2): Linear(in_features=8, out_features=8, bias=True)\n",
       "   (layer1_3): Linear(in_features=8, out_features=1, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pretrain_model(n_hidden, days=30, N_OBS=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebcfc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer():\n",
    "    \"\"\"\n",
    "    S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, N_OBSERVATION):\n",
    "        self.size = size\n",
    "\n",
    "        def blank_array(dim):\n",
    "            assert dim <= 2\n",
    "            dim = N_OBSERVATION if dim == 2 else 1\n",
    "            return np.zeros((size, dim), dtype=np.float32)\n",
    "\n",
    "        self.storage = [blank_array(2), blank_array(2),\n",
    "                        blank_array(1), blank_array(1), blank_array(1), blank_array(1), blank_array(1)]\n",
    "        # order: S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "        self.count = 0\n",
    "\n",
    "    def store(self, values):\n",
    "        index = self.count % self.size\n",
    "        for storage, value in zip(self.storage, values):\n",
    "            storage[index, :] = value\n",
    "        self.count = self.count + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indexes = np.random.choice(self.size, batch_size, False)\n",
    "        return [v[indexes] for v in self.storage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f633393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeBuffer():\n",
    "    def __init__(self, capacity, ep_length, N_OBSERVATION):\n",
    "        self.capacity = capacity\n",
    "        self.ep_count = 0\n",
    "\n",
    "        def blank_array(dim):\n",
    "            assert dim <= 2\n",
    "            dim = N_OBSERVATION if dim == 2 else 1\n",
    "            return np.zeros((capacity, ep_length, dim), dtype=np.float32)\n",
    "\n",
    "        self.storage = [blank_array(2), blank_array(2),\n",
    "                        blank_array(1), blank_array(1), blank_array(1), blank_array(1), blank_array(1)]\n",
    "        # order: S_t0, S_t1, reward, terminated, can_early_exercise, payoff, dS\n",
    "\n",
    "    def store(self, values, t):\n",
    "        ep_index = self.ep_count % self.capacity\n",
    "        for storage, value in zip(self.storage, values):\n",
    "            storage[ep_index, t, :] = value\n",
    "        done = values[3]\n",
    "        if done:\n",
    "            self.ep_count = self.ep_count + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indexes = np.random.choice(self.capacity, batch_size, False)\n",
    "        return [v[indexes] for v in self.storage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d5f094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_episode_wise(env, buffer: EpisodeBuffer, episodes, action=0.5):\n",
    "    for i in range(episodes):\n",
    "        env.reset()\n",
    "        while True:\n",
    "            data = env.step(action)  # actual delta still doesn't matter, avoid calling actor to save time\n",
    "            done = data[3]\n",
    "            buffer.store(data, env.t - 1)\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10ff2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_actor(env, n_hidden, n_samples, epoch=12):\n",
    "    N_OBSERVATION = env.n_observation\n",
    "    days = env.tenor\n",
    "    buffer = EpisodeBuffer(n_samples, days, VanillaEnv.n_observation)\n",
    "    gather_episode_wise(env, buffer, n_samples)\n",
    "    observations, dS, payoff = buffer.storage[0], buffer.storage[-1], buffer.storage[-2][:, -1, 0]\n",
    "    pretrain_actor, pretrainer = get_pretrain_model(n_hidden, days, N_OBSERVATION)  #\n",
    "    pretrainer.compile(loss=tf.keras.losses.mse, optimizer=\"Adam\")\n",
    "    pretrainer.fit((observations, dS), payoff, batch_size = 64, epoch)\n",
    "    actor = get_actor(n_hidden, N_OBSERVATION)\n",
    "    actor.set_weights(pretrain_actor.get_weights())\n",
    "    return actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d6e6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer  = EpisodeBuffer(n_samples, days, VanillaEnv.n_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d7cd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observations, dS, payoff = buffer.storage[0], buffer.storage[-1], buffer.storage[-2][:, -1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2263319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4096, 30, 5), (4096, 30, 1), (4096,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.shape, dS.shape, payoff.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
